"""
RedCube AI å·¥ä½œæµç³»ç»Ÿ - é‡æ„ç‰ˆ
é›†æˆæ–°çš„æ ¸å¿ƒç»„ä»¶ï¼šé…ç½®ç®¡ç†ã€å¼‚å¸¸å¤„ç†ã€ä¾èµ–æ³¨å…¥ã€è¾“å‡ºç®¡ç†

æ ¸å¿ƒæ”¹è¿›ï¼š
1. ä½¿ç”¨ç»Ÿä¸€é…ç½®ç®¡ç†
2. é›†æˆå¼‚å¸¸å¤„ç†æ¡†æ¶
3. é‡‡ç”¨ä¾èµ–æ³¨å…¥æ¨¡å¼
4. ä½¿ç”¨æ··åˆæ•°æ®æµæ¶æ„
5. æ”¹è¿›é”™è¯¯æ¢å¤æœºåˆ¶
"""

import asyncio
from datetime import datetime
from typing import Dict, Any, Optional, List
from pathlib import Path

# æ ¸å¿ƒç»„ä»¶å¯¼å…¥
from modules.core.config import get_config, get_config_value
from modules.core.exceptions import (
    WorkflowException, EngineException, SystemException,
    ErrorCode, get_exception_handler
)
from modules.core.container import get_engine_container, EngineContainer
from modules.core.output import (
    get_output_manager, UnifiedOutput, ContentType, OutputFormat
)

# ä¼ ç»Ÿç»„ä»¶å¯¼å…¥
from modules.utils import get_logger
from modules.models import get_langchain_model
from modules.git_automation import get_git_automation, commit_checkpoint

class BaseWorkflowEngine:
    """é‡æ„åçš„å·¥ä½œæµå¼•æ“åŸºç±»"""
    
    def __init__(self, llm=None, **kwargs):
        # è·å–æ ¸å¿ƒæœåŠ¡
        self.config = get_config()
        self.logger = get_logger(self.__class__.__name__)
        self.exception_handler = get_exception_handler()
        self.output_manager = get_output_manager()
        self.git_auto = get_git_automation() if get_config_value("git.auto_commit", True) else None
        
        # å¼•æ“é…ç½®
        self.engine_name = self.__class__.__name__.lower().replace("engine", "")
        self.llm = llm
        self.cache_enabled = get_config_value("workflow.enable_cache", True)
        self.cache_dir = Path(get_config_value("paths.cache_dir", "cache"))
        
        # åˆå§‹åŒ–å¼•æ“
        self._initialize_engine()
    
    def _initialize_engine(self):
        """åˆå§‹åŒ–å¼•æ“"""
        try:
            # æ£€æŸ¥å¼•æ“æ˜¯å¦å¯ç”¨
            engine_enabled = get_config_value(f"engines.{self.engine_name}.enabled", True)
            if not engine_enabled:
                self.logger.warning(f"å¼•æ“ {self.engine_name} å·²ç¦ç”¨")
                return
            
            # åˆ›å»ºç¼“å­˜ç›®å½•
            engine_cache_dir = self.cache_dir / f"engine_{self.engine_name}"
            engine_cache_dir.mkdir(parents=True, exist_ok=True)
            
            # è°ƒç”¨å­ç±»åˆå§‹åŒ–
            self._setup_engine()
            
            self.logger.info(f"âœ“ {self.engine_name} å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            error_msg = f"å¼•æ“ {self.engine_name} åˆå§‹åŒ–å¤±è´¥"
            self.logger.error(f"{error_msg}: {str(e)}")
            raise EngineException(
                self.engine_name, 
                error_msg, 
                ErrorCode.ENGINE_INIT_FAILED,
                context={"initialization_error": str(e)},
                cause=e
            )
    
    def _setup_engine(self):
        """å­ç±»åº”é‡å†™æ­¤æ–¹æ³•è¿›è¡Œå…·ä½“åˆå§‹åŒ–"""
        pass
    
    def create_output(self, topic: str, content_type: ContentType = ContentType.REPORT) -> UnifiedOutput:
        """åˆ›å»ºç»Ÿä¸€è¾“å‡ºå¯¹è±¡"""
        return self.output_manager.create_output(self.engine_name, topic, content_type)
    
    def load_cache(self, topic: str) -> Optional[UnifiedOutput]:
        """åŠ è½½ç¼“å­˜"""
        if not self.cache_enabled:
            return None
        
        try:
            return self.output_manager.load_output(
                self.engine_name, 
                topic, 
                subdirectory=f"engine_{self.engine_name}"
            )
        except Exception as e:
            self.logger.warning(f"ç¼“å­˜åŠ è½½å¤±è´¥: {str(e)}")
            return None
    
    def save_cache(self, output: UnifiedOutput):
        """ä¿å­˜ç¼“å­˜"""
        if not self.cache_enabled:
            return
        
        try:
            self.output_manager.save_output(
                output, 
                subdirectory=f"engine_{self.engine_name}"
            )
        except Exception as e:
            self.logger.warning(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {str(e)}")
    
    async def execute_with_recovery(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """å¸¦å¼‚å¸¸æ¢å¤çš„æ‰§è¡Œ"""
        topic = inputs.get("topic", "")
        max_retries = get_config_value("error_handling.max_retries", 3)
        retry_delay = get_config_value("error_handling.retry_delay", 1.0)
        
        for attempt in range(max_retries + 1):
            try:
                self.logger.info(f"ğŸ”„ {self.engine_name} å¼•æ“æ‰§è¡Œ (å°è¯• {attempt + 1}/{max_retries + 1})")
                
                # æ‰§è¡Œæ ¸å¿ƒé€»è¾‘
                result = await self.execute(inputs)
                
                # æ£€æŸ¥æ‰§è¡Œç»“æœ
                if result.get("success", True):
                    # è‡ªåŠ¨æäº¤Gitï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if self.git_auto and get_config_value("git.commit_on_engine_complete", True):
                        commit_result = self.git_auto.commit_on_engine_complete(
                            self.engine_name, topic
                        )
                        if commit_result["success"]:
                            result["git_commit"] = commit_result["commit_hash"]
                
                return result
                
            except Exception as e:
                self.logger.error(f"å¼•æ“æ‰§è¡Œå¤±è´¥ (å°è¯• {attempt + 1}): {str(e)}")
                
                # ä½¿ç”¨å¼‚å¸¸å¤„ç†å™¨
                error_result = self.exception_handler.handle_exception(e)
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥é‡è¯•
                if attempt < max_retries and error_result.get("should_retry", False):
                    retry_delay_actual = error_result.get("retry_delay", retry_delay)
                    self.logger.info(f"â³ {retry_delay_actual}ç§’åé‡è¯•...")
                    await asyncio.sleep(retry_delay_actual)
                    continue
                else:
                    # è¿”å›é”™è¯¯ç»“æœ
                    return self._create_error_result(e, error_result)
        
        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        return self._create_error_result(
            Exception(f"å¼•æ“ {self.engine_name} åœ¨ {max_retries} æ¬¡é‡è¯•åä»ç„¶å¤±è´¥"),
            {"success": False, "error": "max_retries_exceeded"}
        )
    
    def _create_error_result(self, exception: Exception, error_result: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºé”™è¯¯ç»“æœ"""
        topic = "unknown"
        
        # åˆ›å»ºé”™è¯¯è¾“å‡º
        error_output = self.create_output(topic, ContentType.REPORT)
        error_output.set_content(
            f"# {self.engine_name} å¼•æ“æ‰§è¡Œå¤±è´¥\n\n## é”™è¯¯ä¿¡æ¯\n{str(exception)}\n\n## é”™è¯¯è¯¦æƒ…\n{error_result}",
            OutputFormat.TEXT
        )
        error_output.set_metadata(
            execution_status="failed",
            error_type=type(exception).__name__,
            error_message=str(exception),
            recovery_attempted=True
        )
        
        return error_output.to_dict()
    
    async def execute(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå¼•æ“é€»è¾‘ - å­ç±»å¿…é¡»å®ç°"""
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç°executeæ–¹æ³•")

class RedCubeWorkflow:
    """é‡æ„åçš„RedCube AIå·¥ä½œæµç³»ç»Ÿ"""
    
    def __init__(self, api_key: Optional[str] = None, model_name: Optional[str] = None):
        # è·å–æ ¸å¿ƒæœåŠ¡
        self.config = get_config()
        self.logger = get_logger("RedCubeWorkflow")
        self.exception_handler = get_exception_handler()
        self.container = get_engine_container()
        self.output_manager = get_output_manager()
        self.git_auto = get_git_automation() if get_config_value("git.auto_commit", True) else None
        
        # åˆå§‹åŒ–AIæ¨¡å‹
        self.llm = self._initialize_llm(api_key, model_name)
        
        # åˆå§‹åŒ–å¼•æ“
        self.engines = {}
        self._initialize_engines()
        
        self.logger.info("ğŸš€ RedCube AI å·¥ä½œæµç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_llm(self, api_key: Optional[str], model_name: Optional[str]):
        """åˆå§‹åŒ–è¯­è¨€æ¨¡å‹"""
        try:
            # ä½¿ç”¨é…ç½®æˆ–å‚æ•°
            final_api_key = api_key or self.config.get("ai.api_key")
            final_model_name = model_name or get_config_value("ai.model_name", "gemini-pro")
            
            if not final_api_key:
                import os
                final_api_key = os.environ.get("GOOGLE_API_KEY")
            
            if not final_api_key:
                raise SystemException(
                    "APIå¯†é’¥æœªé…ç½®",
                    ErrorCode.API_KEY_MISSING,
                    context={"required_env_var": "GOOGLE_API_KEY"}
                )
            
            return get_langchain_model(final_api_key, final_model_name)
            
        except Exception as e:
            raise SystemException(
                f"è¯­è¨€æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}",
                ErrorCode.SYSTEM_INIT_FAILED,
                cause=e
            )
    
    def _initialize_engines(self):
        """åˆå§‹åŒ–8ä¸ªå¼•æ“"""
        self.logger.info("å¼€å§‹åˆå§‹åŒ–RedCube AI 8ä¸ªå¼•æ“...")
        
        engine_configs = [
            ("persona_core_v2", "PersonaCoreEngineV2"),
            ("strategy_compass_v2", "StrategyCompassEngineV2"),
            ("truth_detector_v2", "TruthDetectorEngineV2"),
            ("insight_distiller_v2", "InsightDistillerEngineV2"),
            ("narrative_prism_v2", "NarrativePrismEngineV2"),
            ("atomic_designer_v2", "AtomicDesignerEngineV2"),
            ("visual_encoder_v2", "VisualEncoderEngineV2"),
            ("hifi_imager_v2", "HiFiImagerEngineV2")
        ]
        
        success_count = 0
        
        for engine_name, engine_class_name in engine_configs:
            try:
                # åŸºç¡€å¼•æ“åï¼ˆå»æ‰_v2åç¼€ï¼‰
                base_engine_name = engine_name.replace("_v2", "")
                
                # æ£€æŸ¥å¼•æ“æ˜¯å¦å¯ç”¨
                if not get_config_value(f"engines.{base_engine_name}.enabled", True):
                    self.logger.info(f"â­ï¸ {base_engine_name} å¼•æ“å·²ç¦ç”¨ï¼Œè·³è¿‡")
                    continue
                
                # åŠ¨æ€å¯¼å…¥å¼•æ“
                module_path = f"modules.engines.{engine_name}"
                engine_module = __import__(module_path, fromlist=[engine_class_name])
                engine_class = getattr(engine_module, engine_class_name)
                
                # å®ä¾‹åŒ–å¼•æ“
                self.engines[base_engine_name] = engine_class(self.llm)
                success_count += 1
                
                self.logger.info(f"âœ“ {base_engine_name} å¼•æ“V2åˆå§‹åŒ–æˆåŠŸ")
                
            except Exception as e:
                self.logger.error(f"âœ— {engine_name} å¼•æ“åˆå§‹åŒ–å¤±è´¥: {str(e)}")
                
                # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ç»§ç»­
                fail_fast = get_config_value("error_handling.fail_fast", False)
                if fail_fast:
                    raise WorkflowException(
                        f"å¼•æ“åˆå§‹åŒ–å¤±è´¥: {engine_name}",
                        ErrorCode.ENGINE_INIT_FAILED,
                        context={"failed_engine": engine_name, "error": str(e)},
                        cause=e
                    )
        
        self.logger.info(f"ğŸ¯ å¼•æ“åˆå§‹åŒ–å®Œæˆ: {success_count}/8 ä¸ªå¼•æ“åŠ è½½æˆåŠŸ")
        
        if success_count == 0:
            raise SystemException(
                "æ²¡æœ‰ä»»ä½•å¼•æ“åˆå§‹åŒ–æˆåŠŸ",
                ErrorCode.SYSTEM_INIT_FAILED
            )
    
    async def execute_workflow(self, topic: str, **kwargs) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´å·¥ä½œæµ"""
        self.logger.info(f"ğŸ¬ å¼€å§‹æ‰§è¡ŒRedCube AIå·¥ä½œæµ - ä¸»é¢˜: {topic}")
        
        # åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡
        context = {
            "topic": topic,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "force_regenerate": kwargs.get("force_regenerate", False),
            "enable_git": kwargs.get("enable_git", get_config_value("git.auto_commit", True)),
            "parallel_execution": kwargs.get("parallel_execution", get_config_value("workflow.parallel_engines", False))
        }
        
        try:
            # å·¥ä½œæµå¼€å§‹æ£€æŸ¥ç‚¹
            if context["enable_git"]:
                commit_checkpoint(f"å¼€å§‹å·¥ä½œæµ - {topic}")
            
            # æ‰§è¡Œå¼•æ“é˜¶æ®µ
            if context["parallel_execution"]:
                results = await self._execute_parallel_workflow(context)
            else:
                results = await self._execute_sequential_workflow(context)
            
            # æ„å»ºæœ€ç»ˆç»“æœ
            final_result = self._build_final_result(topic, context, results)
            
            # ä¿å­˜å·¥ä½œæµç»“æœ
            self._save_workflow_result(topic, final_result)
            
            # æœ€ç»ˆæäº¤
            if context["enable_git"]:
                final_commit = self.git_auto.auto_commit(
                    f"å®Œæˆå®Œæ•´å·¥ä½œæµ - {topic}", "feat"
                )
                if final_commit["success"]:
                    final_result["final_commit"] = final_commit["commit_hash"]
            
            self.logger.info("ğŸ‰ RedCube AIå·¥ä½œæµæ‰§è¡Œå®Œæˆ")
            return final_result
            
        except Exception as e:
            self.logger.error(f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}")
            error_result = self.exception_handler.handle_exception(e)
            
            # è¿”å›é”™è¯¯ç»“æœ
            return {
                "success": False,
                "topic": topic,
                "error": error_result,
                "partial_results": context.get("partial_results", {})
            }
    
    async def _execute_sequential_workflow(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """ä¸²è¡Œæ‰§è¡Œå·¥ä½œæµ"""
        topic = context["topic"]
        results = {}
        
        # ç¬¬ä¸€è®¤çŸ¥è±¡é™ï¼šæˆ˜ç•¥æ„æƒ³
        self.logger.info("ğŸ§  ç¬¬ä¸€è®¤çŸ¥è±¡é™ï¼šæˆ˜ç•¥æ„æƒ³é˜¶æ®µ")
        
        # æŒ‰ä¾èµ–é¡ºåºæ‰§è¡Œ
        execution_order = [
            ("persona_core", "äººæ ¼æ ¸å¿ƒ"),
            ("strategy_compass", "ç­–ç•¥ç½—ç›˜"),
            ("truth_detector", "çœŸç†æ¢æœº"),
            ("insight_distiller", "æ´å¯Ÿæç‚¼å™¨")
        ]
        
        for engine_name, engine_desc in execution_order:
            if engine_name in self.engines:
                results[engine_name] = await self._execute_single_engine(
                    engine_name, engine_desc, context, results
                )
        
        # æˆ˜ç•¥é˜¶æ®µæ£€æŸ¥ç‚¹
        if context["enable_git"]:
            commit_checkpoint(f"å®Œæˆæˆ˜ç•¥æ„æƒ³é˜¶æ®µ - {topic}")
        
        # ç¬¬äºŒè®¤çŸ¥è±¡é™ï¼šå™äº‹è¡¨è¾¾
        self.logger.info("ğŸ¨ ç¬¬äºŒè®¤çŸ¥è±¡é™ï¼šå™äº‹è¡¨è¾¾é˜¶æ®µ")
        
        execution_order = [
            ("narrative_prism", "å™äº‹æ£±é•œ"),
            ("atomic_designer", "åŸå­è®¾è®¡å¸ˆ"),
            ("visual_encoder", "è§†è§‰ç¼–ç å™¨"),
            ("hifi_imager", "é«˜ä¿çœŸæˆåƒä»ª")
        ]
        
        for engine_name, engine_desc in execution_order:
            if engine_name in self.engines:
                results[engine_name] = await self._execute_single_engine(
                    engine_name, engine_desc, context, results
                )
        
        return results
    
    async def _execute_parallel_workflow(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """å¹¶è¡Œæ‰§è¡Œå·¥ä½œæµï¼ˆæœªæ¥å®ç°ï¼‰"""
        # ç›®å‰å›é€€åˆ°ä¸²è¡Œæ‰§è¡Œ
        self.logger.info("å¹¶è¡Œæ‰§è¡Œæ¨¡å¼å°šæœªå®ç°ï¼Œå›é€€åˆ°ä¸²è¡Œæ¨¡å¼")
        return await self._execute_sequential_workflow(context)
    
    async def _execute_single_engine(self, engine_name: str, engine_desc: str,
                                   context: Dict[str, Any], 
                                   previous_results: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå•ä¸ªå¼•æ“"""
        self.logger.info(f"ğŸ”§ æ‰§è¡Œ{engine_desc}å¼•æ“...")
        
        engine = self.engines[engine_name]
        
        # å‡†å¤‡å¼•æ“è¾“å…¥
        engine_context = context.copy()
        engine_context.update(previous_results)
        
        # æ‰§è¡Œå¼•æ“
        result = await engine.execute_with_recovery(engine_context)
        
        return result
    
    def _build_final_result(self, topic: str, context: Dict[str, Any], 
                           results: Dict[str, Any]) -> Dict[str, Any]:
        """æ„å»ºæœ€ç»ˆç»“æœ"""
        successful_engines = sum(1 for result in results.values() 
                               if result.get("success", True))
        
        return {
            "workflow": "redcube_ai_v2",
            "version": "2.0.0",
            "topic": topic,
            "timestamp": context["timestamp"],
            "execution_summary": {
                "total_engines": len(self.engines),
                "successful_engines": successful_engines,
                "success_rate": successful_engines / len(self.engines) if self.engines else 0,
                "enable_git": context["enable_git"],
                "parallel_execution": context["parallel_execution"]
            },
            "results": results,
            "success": successful_engines > 0
        }
    
    def _save_workflow_result(self, topic: str, result: Dict[str, Any]):
        """ä¿å­˜å·¥ä½œæµç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = Path(get_config_value("paths.output_dir", "output"))
        workflow_dir = output_dir / f"redcube_{topic}_{timestamp}"
        workflow_dir.mkdir(parents=True, exist_ok=True)
        
        result_file = workflow_dir / "redcube_workflow_result.json"
        import json
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ğŸ“ å·¥ä½œæµç»“æœå·²ä¿å­˜: {result_file}")

# ä¾¿æ·å‡½æ•°
def create_redcube_workflow(api_key: Optional[str] = None, 
                           model_name: Optional[str] = None) -> RedCubeWorkflow:
    """åˆ›å»ºRedCubeå·¥ä½œæµå®ä¾‹"""
    return RedCubeWorkflow(api_key, model_name) 